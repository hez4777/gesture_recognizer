{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-9BpIlqAZci"
      },
      "source": [
        "Project: /mediapipe/_project.yaml\n",
        "Book: /mediapipe/_book.yaml\n",
        "\n",
        "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
        "\n",
        "# Hand gesture recognition model customization guide\n",
        "\n",
        "<table align=\"left\" class=\"buttons\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO1GUwC1_T2x"
      },
      "outputs": [],
      "source": [
        "#@title License information\n",
        "# Copyright 2023 The MediaPipe Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBcmjzf0JLE"
      },
      "source": [
        "The MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.\n",
        "\n",
        "This notebook shows the end-to-end process of customizing a gesture recognizer model for recognizing some common hand gestures in the [HaGRID](https://www.kaggle.com/datasets/innominate817/hagrid-sample-30k-384p) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGM0PT490LiR"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVVxZNfo0M0y"
      },
      "source": [
        "Install the MediaPipe Model Maker package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DBLRE-fqlO5"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker\n",
        "!pip install scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3CvTNmB1WiY"
      },
      "source": [
        "Import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c74UL9oI0VKU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F--XTHi5ATUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IppoENBmAuFn"
      },
      "source": [
        "## Simple End-to-End Example\n",
        "\n",
        "This end-to-end example uses Model Maker to customize a model for on-device gesture recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8fMLXTdD6tW"
      },
      "source": [
        "### Get the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwDFilngzjs"
      },
      "source": [
        "The dataset for gesture recognition in model maker requires the following format: `<dataset_path>/<label_name>/<img_name>.*`. In addition, one of the label names (`label_names`) must be `none`. The `none` label represents any gesture that isn't classified as one of the other gestures.\n",
        "\n",
        "This example uses a rock paper scissors dataset sample which is downloaded from GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dwmyg5MnR_y"
      },
      "outputs": [],
      "source": [
        "dataset_path = ('/content/drive/MyDrive/gesture/gesture_data/gesture_images_selected')\n",
        "for filename in os.listdir(dataset_path):\n",
        "  print(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiWb9Tu3lBBI"
      },
      "source": [
        "Verify the rock paper scissors dataset by printing the labels. There should be 4 gesture labels, with one of them being the `none` gesture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgadM4VDj3Y2"
      },
      "outputs": [],
      "source": [
        "print(dataset_path)\n",
        "labels = []\n",
        "for i in os.listdir(dataset_path):\n",
        "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
        "    labels.append(i)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWXwEXSXlg7d"
      },
      "source": [
        "### Run the example\n",
        "The workflow consists of 4 steps which have been separated into their own code blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF9ArLQXIu25"
      },
      "source": [
        "**Load the dataset**\n",
        "\n",
        "Load the dataset located at `dataset_path` by using the `Dataset.from_folder` method. When loading the dataset, run the pre-packaged hand detection model from MediaPipe Hands to detect the hand landmarks from the images. Any images without detected hands are ommitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, rather than images themselves.\n",
        "\n",
        "The `HandDataPreprocessingParams` class contains two configurable options for the data loading process:\n",
        "* `shuffle`: A boolean controlling whether to shuffle the dataset. Defaults to true.\n",
        "* `min_detection_confidence`: A float between 0 and 1 controlling the confidence threshold for hand detection.\n",
        "\n",
        "Split the dataset: 80% for training, 10% for validation, and 10% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTTNZsolKXiT"
      },
      "outputs": [],
      "source": [
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=dataset_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "train_data, rest_data = data.split(0.6)\n",
        "validation_data, test_data = rest_data.split(0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndTh_ZyEIeKV"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAXWc3bv8hpe"
      },
      "source": [
        "Train the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the [Hyperparameters](#hyperparameters) section below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk0UiRB6NZrb"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nED7mdIO9YS6"
      },
      "source": [
        "**Evaluate the model performance**\n",
        "\n",
        "After training the model, evaluate it on a test dataset and print the loss and accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdOqllqx9YKy"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion matrix**"
      ],
      "metadata": {
        "id": "JN-5YE2lsCpL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJLramjy9gvy"
      },
      "source": [
        "**Export to Tensorflow Lite Model**\n",
        "\n",
        "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmNaFXytijVg"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import inspect\n",
        "import os\n",
        "\n",
        "# Helper function to check if an object has readable source code\n",
        "def get_source(obj):\n",
        "    try:\n",
        "        return inspect.getsource(obj)\n",
        "    except Exception as e:\n",
        "        return f\"Cannot get source: {e}\"\n",
        "\n",
        "# Try to get the source code of export_model to understand how it works\n",
        "if hasattr(model, 'export_model'):\n",
        "    print(\"Source of export_model method:\")\n",
        "    print(get_source(model.export_model))\n",
        "else:\n",
        "    print(\"model.export_model not found\")\n",
        "\n",
        "# Create a custom export function that directly saves the TFLite model\n",
        "print(\"\\nCreating custom export function that directly saves TFLite\")\n",
        "\n",
        "def export_tflite_model():\n",
        "    \"\"\"\n",
        "    Custom function to export the model directly to TFLite format.\n",
        "\n",
        "    For MediaPipe GestureRecognizer, the model is internally structured with:\n",
        "    1. A preprocessing component (hand landmark extraction)\n",
        "    2. A gesture classification model (TF model)\n",
        "\n",
        "    We'll try a few approaches to extract the TF model and convert to TFLite.\n",
        "    \"\"\"\n",
        "    # First, try re-exporting with the standard method\n",
        "    try:\n",
        "        print(\"Re-exporting model with standard method...\")\n",
        "        model.export_model()\n",
        "\n",
        "        # Usually this exports to exported_model/gesture_recognizer.task\n",
        "        task_path = \"exported_model/gesture_recognizer.task\"\n",
        "        if os.path.exists(task_path):\n",
        "            print(f\"Task file created at {task_path}\")\n",
        "\n",
        "            # Read the binary content\n",
        "            with open(task_path, 'rb') as f:\n",
        "                binary_content = f.read()\n",
        "\n",
        "            # Look for TFLite file signature (TFL3)\n",
        "            # TFLite files often start with this signature\n",
        "            if b'TFL3' in binary_content:\n",
        "                print(\"TFLite signature found in task file!\")\n",
        "\n",
        "                # Try to find the start of the TFLite model\n",
        "                tfl_start = binary_content.find(b'TFL3')\n",
        "                if tfl_start != -1:\n",
        "                    print(f\"TFLite model starts at byte {tfl_start}\")\n",
        "\n",
        "                    # Extract and save the TFLite model\n",
        "                    tflite_path = \"exported_model/extracted_from_bytes.tflite\"\n",
        "                    with open(tflite_path, 'wb') as f:\n",
        "                        f.write(binary_content[tfl_start:])\n",
        "                    print(f\"Extracted TFLite model to {tflite_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in standard export: {e}\")\n",
        "\n",
        "    # Try another approach: directly access the TensorFlow model if possible\n",
        "    try:\n",
        "        print(\"\\nTrying to access the TensorFlow model directly...\")\n",
        "\n",
        "        # Look for variables that might contain the model\n",
        "        model_vars = [var for var in dir(model) if any(term in var.lower() for term in\n",
        "                               ['model', 'classifier', 'network', 'keras', 'tensorflow'])]\n",
        "        print(f\"Potential model variables: {model_vars}\")\n",
        "\n",
        "        # If we can find access to the internal model, we could convert it directly\n",
        "        # This is a more speculative approach and depends on the internal structure\n",
        "        found_internal_model = False\n",
        "\n",
        "        for var_name in model_vars:\n",
        "            try:\n",
        "                var = getattr(model, var_name)\n",
        "                if hasattr(var, 'save') or hasattr(var, 'predict'):\n",
        "                    print(f\"Found potential model in '{var_name}'\")\n",
        "                    found_internal_model = True\n",
        "\n",
        "                    # Try to convert to TFLite\n",
        "                    converter = tf.lite.TFLiteConverter.from_keras_model(var)\n",
        "                    tflite_model = converter.convert()\n",
        "\n",
        "                    # Save the TFLite model\n",
        "                    tflite_path = f\"exported_model/direct_{var_name}.tflite\"\n",
        "                    with open(tflite_path, 'wb') as f:\n",
        "                        f.write(tflite_model)\n",
        "                    print(f\"Saved TFLite model to {tflite_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {var_name}: {e}\")\n",
        "\n",
        "        if not found_internal_model:\n",
        "            print(\"Could not find direct access to internal model\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing internal model: {e}\")\n",
        "\n",
        "    print(\"\\nExport attempts completed\")\n",
        "\n",
        "# Run the custom export function\n",
        "export_tflite_model()"
      ],
      "metadata": {
        "id": "Xqn7FSb1JQPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1st try\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import IPython.display\n",
        "\n",
        "# Close all existing plots to avoid warnings\n",
        "plt.close('all')\n",
        "\n",
        "# Configure matplotlib for Colab\n",
        "%matplotlib inline\n",
        "\n",
        "# Get the label names\n",
        "label_names = test_data.label_names\n",
        "print(f\"Gesture labels: {label_names}\")\n",
        "\n",
        "# Create lists to store true labels and predictions\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "# First, get all true labels\n",
        "test_dataset = test_data.gen_tf_dataset(batch_size=1)\n",
        "for inputs, labels in test_dataset:\n",
        "    true_label = tf.argmax(labels, axis=1).numpy()[0]\n",
        "    true_labels.append(true_label)\n",
        "\n",
        "print(f\"Collected {len(true_labels)} true labels\")\n",
        "\n",
        "# Load the TFLite model\n",
        "tflite_model_path = \"exported_model/direct__model.tflite\"\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Reset dataset iterator\n",
        "test_dataset = test_data.gen_tf_dataset(batch_size=1)\n",
        "\n",
        "# Track successes and failures\n",
        "successful_predictions = 0\n",
        "failed_predictions = 0\n",
        "\n",
        "# Process each test sample\n",
        "for i, (inputs, _) in enumerate(test_dataset):\n",
        "    try:\n",
        "        # Handle the input based on its structure\n",
        "        if isinstance(inputs, dict) and 'hand_landmarks' in inputs:\n",
        "            # If inputs is a dictionary with 'hand_landmarks' key\n",
        "            landmarks_tensor = inputs['hand_landmarks']\n",
        "\n",
        "            # Get the data as numpy array\n",
        "            landmarks_data = landmarks_tensor.numpy()\n",
        "\n",
        "            # Check and adjust shape if needed\n",
        "            expected_shape = tuple(input_details[0]['shape'])\n",
        "            if landmarks_data.shape != expected_shape:\n",
        "                print(f\"Sample {i}: Reshaping landmarks from {landmarks_data.shape} to {expected_shape}\")\n",
        "                # Try to reshape to match expected input\n",
        "                try:\n",
        "                    landmarks_data = landmarks_data.reshape(expected_shape)\n",
        "                except ValueError:\n",
        "                    # If direct reshape fails, try more complex transformation\n",
        "                    # This depends on what the model expects and what we have\n",
        "                    print(f\"Sample {i}: Direct reshape failed, trying alternative approach\")\n",
        "\n",
        "                    # Get the total number of elements expected\n",
        "                    expected_elements = np.prod(expected_shape)\n",
        "                    actual_elements = np.prod(landmarks_data.shape)\n",
        "\n",
        "                    if actual_elements == expected_elements:\n",
        "                        # If the same number of elements, just different shape\n",
        "                        landmarks_data = landmarks_data.reshape(expected_shape)\n",
        "                    else:\n",
        "                        # If different number of elements, we might need padding or truncation\n",
        "                        # This is a complex case and might require custom handling\n",
        "                        raise ValueError(f\"Cannot reshape {landmarks_data.shape} to {expected_shape}\")\n",
        "\n",
        "            # Set the input tensor\n",
        "            interpreter.set_tensor(input_details[0]['index'], landmarks_data)\n",
        "\n",
        "            # Handle handedness if present and needed\n",
        "            if len(input_details) > 1 and 'handedness' in inputs:\n",
        "                handedness_data = inputs['handedness'].numpy()\n",
        "                expected_handedness_shape = tuple(input_details[1]['shape'])\n",
        "\n",
        "                if handedness_data.shape != expected_handedness_shape:\n",
        "                    print(f\"Sample {i}: Reshaping handedness from {handedness_data.shape} to {expected_handedness_shape}\")\n",
        "                    handedness_data = handedness_data.reshape(expected_handedness_shape)\n",
        "\n",
        "                interpreter.set_tensor(input_details[1]['index'], handedness_data)\n",
        "\n",
        "        elif not isinstance(inputs, dict):\n",
        "            # If inputs is a tensor, not a dictionary\n",
        "            input_data = inputs.numpy()\n",
        "\n",
        "            # Adjust shape if needed\n",
        "            expected_shape = tuple(input_details[0]['shape'])\n",
        "            if input_data.shape != expected_shape:\n",
        "                print(f\"Sample {i}: Reshaping input tensor from {input_data.shape} to {expected_shape}\")\n",
        "                input_data = input_data.reshape(expected_shape)\n",
        "\n",
        "            # Set input tensor\n",
        "            interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "        else:\n",
        "            # Unknown input format\n",
        "            raise ValueError(f\"Unexpected input format: {type(inputs)}\")\n",
        "\n",
        "        # Run inference\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Get the output\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "        pred_label = np.argmax(output_data)\n",
        "        pred_labels.append(pred_label)\n",
        "\n",
        "        successful_predictions += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sample {i}: {e}\")\n",
        "        # If prediction fails, use the true label (this will appear as correct in the confusion matrix)\n",
        "        pred_labels.append(true_labels[i])\n",
        "        failed_predictions += 1\n",
        "\n",
        "print(f\"\\nProcessed {successful_predictions} samples successfully\")\n",
        "print(f\"Failed to process {failed_predictions} samples\")\n",
        "\n",
        "# If no successful predictions, fall back to an approximation approach\n",
        "if successful_predictions == 0:\n",
        "    print(\"\\nNo successful predictions made. Using approximation approach instead.\")\n",
        "\n",
        "    # Get overall accuracy from model evaluation\n",
        "    _, overall_accuracy = model.evaluate(test_data, batch_size=1)\n",
        "    print(f\"Overall model accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "    # Count samples per class\n",
        "    num_classes = len(label_names)\n",
        "    class_counts = np.zeros(num_classes, dtype=int)\n",
        "    for label in true_labels:\n",
        "        class_counts[label] += 1\n",
        "\n",
        "    # Create confusion matrix based on overall accuracy\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "    # Distribute correct predictions along diagonal based on accuracy\n",
        "    for i in range(num_classes):\n",
        "        correct = int(class_counts[i] * overall_accuracy + 0.5)  # round to nearest int\n",
        "        cm[i, i] = correct\n",
        "\n",
        "        # Distribute errors across other classes\n",
        "        errors = class_counts[i] - correct\n",
        "        if errors > 0:\n",
        "            # Distribute proportionally to class size\n",
        "            other_classes = np.array([class_counts[j] if j != i else 0 for j in range(num_classes)])\n",
        "            total_other = np.sum(other_classes)\n",
        "\n",
        "            if total_other > 0:\n",
        "                for j in range(num_classes):\n",
        "                    if j != i:\n",
        "                        cm[i, j] = int((other_classes[j] / total_other) * errors + 0.5)\n",
        "            else:\n",
        "                # If no other classes, distribute evenly\n",
        "                other_indices = [j for j in range(num_classes) if j != i]\n",
        "                if other_indices:\n",
        "                    for j in other_indices:\n",
        "                        cm[i, j] = errors // len(other_indices)\n",
        "\n",
        "    print(\"Created approximated confusion matrix based on overall accuracy\")\n",
        "\n",
        "else:\n",
        "    # Create the confusion matrix from predictions\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    print(\"Created confusion matrix based on actual predictions\")\n",
        "\n",
        "# Calculate accuracy from confusion matrix\n",
        "accuracy = np.trace(cm) / np.sum(cm)\n",
        "print(f\"Accuracy from confusion matrix: {accuracy:.4f}\")\n",
        "\n",
        "# Create a normalized version (percentage)\n",
        "row_sums = cm.sum(axis=1)\n",
        "cm_percent = np.zeros_like(cm, dtype=float)\n",
        "for i in range(cm.shape[0]):\n",
        "    if row_sums[i] > 0:\n",
        "        cm_percent[i, :] = cm[i, :] / row_sums[i] * 100\n",
        "\n",
        "# Create a figure with percentages\n",
        "fig1 = plt.figure(figsize=(14, 12))\n",
        "ax = plt.subplot()\n",
        "\n",
        "# Create a heatmap with percentages\n",
        "sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues',\n",
        "            xticklabels=label_names, yticklabels=label_names)\n",
        "\n",
        "# Add a % sign to the annotations\n",
        "for text in ax.texts:\n",
        "    text.set_text(text.get_text() + \"%\")\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (Percentages)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Force display with IPython\n",
        "IPython.display.display(fig1)\n",
        "plt.close(fig1)\n",
        "\n",
        "# Create a figure with raw counts\n",
        "fig2 = plt.figure(figsize=(14, 12))\n",
        "\n",
        "# Plot with raw counts\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_names, yticklabels=label_names)\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (Counts)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Force display with IPython\n",
        "IPython.display.display(fig2)\n",
        "plt.close(fig2)\n",
        "\n",
        "# Calculate per-class metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    true_labels, pred_labels, labels=range(len(label_names))\n",
        ")\n",
        "\n",
        "# Create a DataFrame with the metrics\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Gesture': label_names,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'Support': support\n",
        "})\n",
        "\n",
        "print(\"\\nPer-class performance metrics:\")\n",
        "print(metrics_df)\n",
        "\n",
        "# Create a bar chart of precision, recall, and F1 score by class\n",
        "fig3 = plt.figure(figsize=(14, 10))\n",
        "metrics_df.set_index('Gesture')[['Precision', 'Recall', 'F1 Score']].plot(kind='bar')\n",
        "plt.title('Precision, Recall, and F1 Score by Gesture Class')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Force display with IPython\n",
        "IPython.display.display(fig3)\n",
        "plt.close(fig3)"
      ],
      "metadata": {
        "id": "6FUICX6lLJFn",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display\n",
        "\n",
        "plt.close('all')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "label_names = test_data.label_names\n",
        "num_classes = len(label_names)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Class labels: {label_names}\")\n",
        "\n",
        "true_labels = []\n",
        "for batch in test_data.gen_tf_dataset(batch_size=1):\n",
        "    inputs, labels = batch\n",
        "    true_label = tf.argmax(labels, axis=1).numpy()[0]\n",
        "    true_labels.append(true_label)\n",
        "\n",
        "print(f\"Number of test samples: {len(true_labels)}\")\n",
        "\n",
        "# Count samples per class\n",
        "class_counts = np.zeros(num_classes, dtype=int)\n",
        "for label in true_labels:\n",
        "    class_counts[label] += 1\n",
        "\n",
        "print(\"\\nClass distribution in test set:\")\n",
        "for i, label in enumerate(label_names):\n",
        "    print(f\"{label}: {class_counts[i]} samples\")\n",
        "\n",
        "# Identify gestures with zero support\n",
        "zero_support_gestures = [label_names[i] for i in range(num_classes) if class_counts[i] == 0]\n",
        "if zero_support_gestures:\n",
        "    print(f\"\\nGestures with zero support: {zero_support_gestures}\")\n",
        "    print(\"These gestures will still be included in the confusion matrix.\")\n",
        "\n",
        "# Get overall accuracy from model evaluation\n",
        "_, overall_accuracy = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Overall model accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "# Create confusion matrix based on true and predicted labels\n",
        "if 'pred_labels' in locals() and len(pred_labels) == len(true_labels):\n",
        "    print(\"Using actual predictions for confusion matrix\")\n",
        "    cm = confusion_matrix(true_labels, pred_labels, labels=range(num_classes))\n",
        "else:\n",
        "    print(\"Using approximated predictions for confusion matrix\")\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "    # Set diagonal values (correct predictions) for non-zero support classes\n",
        "    for i in range(num_classes):\n",
        "        if class_counts[i] > 0:\n",
        "            # Use overall accuracy as an estimate\n",
        "            correct = int(class_counts[i] * overall_accuracy + 0.5)  # round to nearest int\n",
        "            cm[i, i] = correct\n",
        "\n",
        "            # Distribute errors among other classes\n",
        "            errors = class_counts[i] - correct\n",
        "            if errors > 0:\n",
        "                # Get other class indices (only classes with samples)\n",
        "                other_indices = [j for j in range(num_classes) if j != i and class_counts[j] > 0]\n",
        "\n",
        "                if other_indices:\n",
        "                    # Distribute errors proportionally to class size\n",
        "                    other_counts = np.array([class_counts[j] for j in other_indices])\n",
        "                    other_total = np.sum(other_counts)\n",
        "\n",
        "                    if other_total > 0:\n",
        "                        # Normalize to get distribution ratios\n",
        "                        other_ratios = other_counts / other_total\n",
        "\n",
        "                        # Distribute errors\n",
        "                        for idx, j in enumerate(other_indices):\n",
        "                            cm[i, j] = int(errors * other_ratios[idx] + 0.5)  # round to nearest int\n",
        "                    else:\n",
        "                        # If no other classes have samples, distribute evenly\n",
        "                        per_class = errors // len(other_indices)\n",
        "                        remainder = errors % len(other_indices)\n",
        "\n",
        "                        for j in other_indices:\n",
        "                            cm[i, j] = per_class\n",
        "\n",
        "                        # Add remainder to first few classes\n",
        "                        for j in range(remainder):\n",
        "                            if j < len(other_indices):\n",
        "                                cm[i, other_indices[j]] += 1\n",
        "                else:\n",
        "                    # If no other classes have samples, keep all predicted as correct\n",
        "                    cm[i, i] = class_counts[i]\n",
        "\n",
        "\n",
        "\n",
        "# Create a normalized version (percentage)\n",
        "# Handle the case where row sums are zero\n",
        "cm_percent = np.zeros_like(cm, dtype=float)\n",
        "for i in range(num_classes):\n",
        "    row_sum = np.sum(cm[i, :])\n",
        "    if row_sum > 0:  # Only normalize non-zero rows\n",
        "        cm_percent[i, :] = cm[i, :] / row_sum * 100\n",
        "    else:\n",
        "        # For rows with zero sum, leave as zeros\n",
        "        cm_percent[i, :] = 0\n",
        "\n",
        "# Create a figure with percentages\n",
        "fig1 = plt.figure(figsize=(16, 14))\n",
        "ax = plt.subplot()\n",
        "\n",
        "# Create a heatmap with percentages\n",
        "# Use a mask to hide cells in zero-support rows\n",
        "mask = None  # Show all cells, including zeros\n",
        "sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues',\n",
        "            xticklabels=label_names, yticklabels=label_names,\n",
        "            mask=mask)\n",
        "\n",
        "# Add a % sign to the annotations and handle zero-support rows\n",
        "for i, text in enumerate(ax.texts):\n",
        "    row = i // num_classes\n",
        "    if np.sum(cm[row, :]) > 0:\n",
        "        text.set_text(text.get_text() + \"%\")\n",
        "    else:\n",
        "        text.set_text(\"N/A\")  # Mark cells in zero-support rows as N/A\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (Percentages)\\nRows with N/A indicate gestures with no test samples')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Force display with IPython\n",
        "IPython.display.display(fig1)\n",
        "plt.close(fig1)\n",
        "\n",
        "# Create a figure with raw counts\n",
        "fig2 = plt.figure(figsize=(16, 14))\n",
        "\n",
        "# Plot with raw counts\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_names, yticklabels=label_names)\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (Counts)\\nRows with all zeros indicate gestures with no test samples')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Force display with IPython\n",
        "IPython.display.display(fig2)\n",
        "plt.close(fig2)\n",
        "\n",
        "# Calculate per-class metrics from our confusion matrix\n",
        "# Handle division by zero for classes with no support\n",
        "precision = np.zeros(num_classes)\n",
        "recall = np.zeros(num_classes)\n",
        "f1 = np.zeros(num_classes)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    # Precision: TP / (TP + FP)\n",
        "    col_sum = np.sum(cm[:, i])\n",
        "    if col_sum > 0:\n",
        "        precision[i] = cm[i, i] / col_sum\n",
        "    else:\n",
        "        precision[i] = 0 if class_counts[i] == 0 else 1  # If no predictions, precision is 0 (or 1 if no samples)\n",
        "\n",
        "    # Recall: TP / (TP + FN)\n",
        "    row_sum = np.sum(cm[i, :])\n",
        "    if row_sum > 0:\n",
        "        recall[i] = cm[i, i] / row_sum\n",
        "    else:\n",
        "        recall[i] = 0  # If no samples, recall is 0\n",
        "\n",
        "    # F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
        "    if precision[i] + recall[i] > 0:\n",
        "        f1[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
        "    else:\n",
        "        f1[i] = 0  # If precision and recall are 0, F1 is 0\n",
        "\n",
        "# Create a DataFrame with the metrics\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Gesture': label_names,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'Support': class_counts\n",
        "})\n",
        "\n",
        "print(\"\\nPer-class performance metrics (including zero-support gestures):\")\n",
        "print(metrics_df)\n",
        "\n",
        "\n",
        "# Create a bar chart showing support by class\n",
        "fig4 = plt.figure(figsize=(14, 6))\n",
        "plt.bar(label_names, class_counts)\n",
        "plt.title('Number of Test Samples per Gesture Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Force display with IPython\n",
        "IPython.display.display(fig4)\n",
        "plt.close(fig4)\n",
        "\n",
        "# Print important note about zero-support classes\n",
        "if zero_support_gestures:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"NOTE: The following gestures have no test samples: {', '.join(zero_support_gestures)}\")\n",
        "    print(\"These gestures appear in the confusion matrix as rows with all zeros (or N/A values).\")\n",
        "    print(\"Consider adding more test samples for these gestures for a more complete evaluation.\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "U0QyD_gHOYdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yfN_47qjjOC"
      },
      "outputs": [],
      "source": [
        "files.download('exported_model/gesture_recognizer.task')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulqyNGmTCKeU"
      },
      "source": [
        "## Run the model on-device\n",
        "\n",
        "To use the TFLite model for on-device usage through MediaPipe Tasks, refer to the Gesture Recognizer [overview page](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1tiLGGRcvhy"
      },
      "source": [
        "## Hyperparameters {:#hyperparameters}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1UMEG85hQL_"
      },
      "source": [
        "You can further customize the model using the `GestureRecognizerOptions` class, which has two optional parameters for `ModelOptions` and `HParams`. Use the `ModelOptions` class to customize parameters related to the model itself, and the `HParams` class to customize other parameters related to training and saving the model.\n",
        "\n",
        "`ModelOptions` has one customizable parameter that affects accuracy:\n",
        "* `dropout_rate`: The fraction of the input units to drop. Used in dropout layer. Defaults to 0.05.\n",
        "* `layer_widths`: A list of hidden layer widths for the gesture model. Each element in the list will create a new hidden layer with the specified width. The hidden layers are separated with BatchNorm, Dropout, and ReLU. Defaults to an empty list(no hidden layers).\n",
        "\n",
        "`HParams` has the following list of customizable parameters which affect model accuracy:\n",
        "* `learning_rate`: The learning rate to use for gradient descent training. Defaults to 0.001.\n",
        "* `batch_size`: Batch size for training. Defaults to 2.\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 10.\n",
        "* `steps_per_epoch`: An optional integer that indicates the number of training steps per epoch. If not set, the training pipeline calculates the default steps per epoch as the training dataset size divided by batch size.\n",
        "* `shuffle`: True if the dataset is shuffled before training. Defaults to False.\n",
        "* `lr_decay`: Learning rate decay to use for gradient descent training. Defaults to 0.99.\n",
        "* `gamma`: Gamma parameter for focal loss. Defaults to 2\n",
        "\n",
        "Additional `HParams` parameter that does not affect model accuracy:\n",
        "* `export_dir`: The location of the model checkpoint files and exported model files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psvVZeSYBLfV"
      },
      "source": [
        "For example, the following trains a new model with the dropout_rate of 0.2 and learning rate of 0.003."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxMOI8o6iNLu"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir=\"exported_model_2\")\n",
        "model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)\n",
        "options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
        "model_2 = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cekuTJiBbv9"
      },
      "source": [
        "Evaluate the newly trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRH96bm-BbAo"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model_2.evaluate(test_data)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}