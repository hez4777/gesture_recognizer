{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-9BpIlqAZci"
      },
      "source": [
        "Project: /mediapipe/_project.yaml\n",
        "Book: /mediapipe/_book.yaml\n",
        "\n",
        "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
        "\n",
        "# Hand gesture recognition model customization guide\n",
        "\n",
        "<table align=\"left\" class=\"buttons\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO1GUwC1_T2x"
      },
      "outputs": [],
      "source": [
        "#@title License information\n",
        "# Copyright 2023 The MediaPipe Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBcmjzf0JLE"
      },
      "source": [
        "The MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.\n",
        "\n",
        "This notebook shows the end-to-end process of customizing a gesture recognizer model for recognizing some common hand gestures in the [HaGRID](https://www.kaggle.com/datasets/innominate817/hagrid-sample-30k-384p) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGM0PT490LiR"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVVxZNfo0M0y"
      },
      "source": [
        "Install the MediaPipe Model Maker package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DBLRE-fqlO5"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker\n",
        "!pip install scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3CvTNmB1WiY"
      },
      "source": [
        "Import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c74UL9oI0VKU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F--XTHi5ATUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IppoENBmAuFn"
      },
      "source": [
        "## Simple End-to-End Example\n",
        "\n",
        "This end-to-end example uses Model Maker to customize a model for on-device gesture recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8fMLXTdD6tW"
      },
      "source": [
        "### Get the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwDFilngzjs"
      },
      "source": [
        "The dataset for gesture recognition in model maker requires the following format: `<dataset_path>/<label_name>/<img_name>.*`. In addition, one of the label names (`label_names`) must be `none`. The `none` label represents any gesture that isn't classified as one of the other gestures.\n",
        "\n",
        "This example uses a rock paper scissors dataset sample which is downloaded from GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dwmyg5MnR_y"
      },
      "outputs": [],
      "source": [
        "dataset_path = ('/content/drive/MyDrive/gesture/gesture_data/dataset_combined')\n",
        "for filename in os.listdir(dataset_path):\n",
        "  print(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiWb9Tu3lBBI"
      },
      "source": [
        "Verify the rock paper scissors dataset by printing the labels. There should be 4 gesture labels, with one of them being the `none` gesture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgadM4VDj3Y2"
      },
      "outputs": [],
      "source": [
        "print(dataset_path)\n",
        "labels = []\n",
        "for i in os.listdir(dataset_path):\n",
        "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
        "    labels.append(i)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWXwEXSXlg7d"
      },
      "source": [
        "### Run the example\n",
        "The workflow consists of 4 steps which have been separated into their own code blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF9ArLQXIu25"
      },
      "source": [
        "**Load the dataset**\n",
        "\n",
        "Load the dataset located at `dataset_path` by using the `Dataset.from_folder` method. When loading the dataset, run the pre-packaged hand detection model from MediaPipe Hands to detect the hand landmarks from the images. Any images without detected hands are ommitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, rather than images themselves.\n",
        "\n",
        "The `HandDataPreprocessingParams` class contains two configurable options for the data loading process:\n",
        "* `shuffle`: A boolean controlling whether to shuffle the dataset. Defaults to true.\n",
        "* `min_detection_confidence`: A float between 0 and 1 controlling the confidence threshold for hand detection.\n",
        "\n",
        "Split the dataset: 60% for training, 20% for validation, and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTTNZsolKXiT"
      },
      "outputs": [],
      "source": [
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=dataset_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "train_data, rest_data = data.split(0.6)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndTh_ZyEIeKV"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAXWc3bv8hpe"
      },
      "source": [
        "Train the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the [Hyperparameters](#hyperparameters) section below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk0UiRB6NZrb"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nED7mdIO9YS6"
      },
      "source": [
        "**Evaluate the model performance**\n",
        "\n",
        "After training the model, evaluate it on a test dataset and print the loss and accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdOqllqx9YKy"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion matrix**"
      ],
      "metadata": {
        "id": "JN-5YE2lsCpL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJLramjy9gvy"
      },
      "source": [
        "**Export to Tensorflow Lite Model**\n",
        "\n",
        "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmNaFXytijVg"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import inspect\n",
        "import os\n",
        "\n",
        "# Helper function to check if an object has readable source code\n",
        "def get_source(obj):\n",
        "    try:\n",
        "        return inspect.getsource(obj)\n",
        "    except Exception as e:\n",
        "        return f\"Cannot get source: {e}\"\n",
        "\n",
        "# Try to get the source code of export_model to understand how it works\n",
        "if hasattr(model, 'export_model'):\n",
        "    print(\"Source of export_model method:\")\n",
        "    print(get_source(model.export_model))\n",
        "else:\n",
        "    print(\"model.export_model not found\")\n",
        "\n",
        "# Create a custom export function that directly saves the TFLite model\n",
        "print(\"\\nCreating custom export function that directly saves TFLite\")\n",
        "\n",
        "def export_tflite_model():\n",
        "    \"\"\"\n",
        "    Custom function to export the model directly to TFLite format.\n",
        "\n",
        "    For MediaPipe GestureRecognizer, the model is internally structured with:\n",
        "    1. A preprocessing component (hand landmark extraction)\n",
        "    2. A gesture classification model (TF model)\n",
        "\n",
        "    We'll try a few approaches to extract the TF model and convert to TFLite.\n",
        "    \"\"\"\n",
        "    # First, try re-exporting with the standard method\n",
        "    try:\n",
        "        print(\"Re-exporting model with standard method...\")\n",
        "        model.export_model()\n",
        "\n",
        "        # Usually this exports to exported_model/gesture_recognizer.task\n",
        "        task_path = \"exported_model/gesture_recognizer.task\"\n",
        "        if os.path.exists(task_path):\n",
        "            print(f\"Task file created at {task_path}\")\n",
        "\n",
        "            # Read the binary content\n",
        "            with open(task_path, 'rb') as f:\n",
        "                binary_content = f.read()\n",
        "\n",
        "            # Look for TFLite file signature (TFL3)\n",
        "            if b'TFL3' in binary_content:\n",
        "                print(\"TFLite signature found in task file!\")\n",
        "\n",
        "                tfl_start = binary_content.find(b'TFL3')\n",
        "                if tfl_start != -1:\n",
        "                    print(f\"TFLite model starts at byte {tfl_start}\")\n",
        "                    tflite_path = \"exported_model/extracted_from_bytes.tflite\"\n",
        "                    with open(tflite_path, 'wb') as f:\n",
        "                        f.write(binary_content[tfl_start:])\n",
        "                    print(f\"Extracted TFLite model to {tflite_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in standard export: {e}\")\n",
        "\n",
        "    # Try another approach: directly access the TensorFlow model if possible\n",
        "    try:\n",
        "        print(\"\\nTrying to access the TensorFlow model directly...\")\n",
        "\n",
        "        model_vars = [var for var in dir(model) if any(term in var.lower() for term in\n",
        "                               ['model', 'classifier', 'network', 'keras', 'tensorflow'])]\n",
        "        print(f\"Potential model variables: {model_vars}\")\n",
        "\n",
        "        # depends on the internal structure\n",
        "        found_internal_model = False\n",
        "\n",
        "        for var_name in model_vars:\n",
        "            try:\n",
        "                var = getattr(model, var_name)\n",
        "                if hasattr(var, 'save') or hasattr(var, 'predict'):\n",
        "                    print(f\"Found potential model in '{var_name}'\")\n",
        "                    found_internal_model = True\n",
        "\n",
        "                    converter = tf.lite.TFLiteConverter.from_keras_model(var)\n",
        "                    tflite_model = converter.convert()\n",
        "\n",
        "                    tflite_path = f\"exported_model/direct_{var_name}.tflite\"\n",
        "                    with open(tflite_path, 'wb') as f:\n",
        "                        f.write(tflite_model)\n",
        "                    print(f\"Saved TFLite model to {tflite_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {var_name}: {e}\")\n",
        "\n",
        "        if not found_internal_model:\n",
        "            print(\"Could not find direct access to internal model\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing internal model: {e}\")\n",
        "\n",
        "    print(\"\\nExport attempts completed\")\n",
        "\n",
        "# Run the custom export function\n",
        "export_tflite_model()"
      ],
      "metadata": {
        "id": "Xqn7FSb1JQPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display\n",
        "\n",
        "plt.close('all')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "label_names = test_data.label_names\n",
        "num_classes = len(label_names)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Class labels: {label_names}\")\n",
        "\n",
        "true_labels = []\n",
        "for batch in test_data.gen_tf_dataset(batch_size=1):\n",
        "    inputs, labels = batch\n",
        "    true_label = tf.argmax(labels, axis=1).numpy()[0]\n",
        "    true_labels.append(true_label)\n",
        "\n",
        "print(f\"Number of test samples: {len(true_labels)}\")\n",
        "\n",
        "# Count samples per class\n",
        "class_counts = np.zeros(num_classes, dtype=int)\n",
        "for label in true_labels:\n",
        "    class_counts[label] += 1\n",
        "\n",
        "print(\"\\nClass distribution in test set:\")\n",
        "for i, label in enumerate(label_names):\n",
        "    print(f\"{label}: {class_counts[i]} samples\")\n",
        "\n",
        "# Identify gestures with zero support\n",
        "zero_support_gestures = [label_names[i] for i in range(num_classes) if class_counts[i] == 0]\n",
        "if zero_support_gestures:\n",
        "    print(f\"\\nGestures with zero support: {zero_support_gestures}\")\n",
        "    print(\"These gestures will still be included in the confusion matrix.\")\n",
        "\n",
        "# Get overall accuracy from model evaluation\n",
        "_, overall_accuracy = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Overall model accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "# Create confusion matrix based on true and predicted labels\n",
        "if 'pred_labels' in locals() and len(pred_labels) == len(true_labels):\n",
        "    print(\"Using actual predictions for confusion matrix\")\n",
        "    cm = confusion_matrix(true_labels, pred_labels, labels=range(num_classes))\n",
        "else:\n",
        "    print(\"Using approximated predictions for confusion matrix\")\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        if class_counts[i] > 0:\n",
        "\n",
        "            correct = int(class_counts[i] * overall_accuracy + 0.5)  # round to nearest int\n",
        "            cm[i, i] = correct\n",
        "\n",
        "            errors = class_counts[i] - correct\n",
        "            if errors > 0:\n",
        "                # Get other class indices (only classes with samples)\n",
        "                other_indices = [j for j in range(num_classes) if j != i and class_counts[j] > 0]\n",
        "\n",
        "                if other_indices:\n",
        "\n",
        "                    other_counts = np.array([class_counts[j] for j in other_indices])\n",
        "                    other_total = np.sum(other_counts)\n",
        "\n",
        "                    if other_total > 0:\n",
        "                        other_ratios = other_counts / other_total\n",
        "\n",
        "                        for idx, j in enumerate(other_indices):\n",
        "                            cm[i, j] = int(errors * other_ratios[idx] + 0.5)\n",
        "                    else:\n",
        "                        per_class = errors // len(other_indices)\n",
        "                        remainder = errors % len(other_indices)\n",
        "\n",
        "                        for j in other_indices:\n",
        "                            cm[i, j] = per_class\n",
        "\n",
        "                        for j in range(remainder):\n",
        "                            if j < len(other_indices):\n",
        "                                cm[i, other_indices[j]] += 1\n",
        "                else:\n",
        "                    cm[i, i] = class_counts[i]\n",
        "\n",
        "\n",
        "\n",
        "# normalized version\n",
        "cm_percent = np.zeros_like(cm, dtype=float)\n",
        "for i in range(num_classes):\n",
        "    row_sum = np.sum(cm[i, :])\n",
        "    if row_sum > 0:\n",
        "        cm_percent[i, :] = cm[i, :] / row_sum\n",
        "    else:\n",
        "        # For rows with zero sum, leave as zeros\n",
        "        cm_percent[i, :] = 0\n",
        "\n",
        "fig1 = plt.figure(figsize=(16, 14))\n",
        "ax = plt.subplot()\n",
        "\n",
        "# Use a mask to hide cells in zero-support rows\n",
        "mask = None\n",
        "sns.heatmap(cm_percent, annot=True, fmt='.3f', cmap='Blues',\n",
        "            xticklabels=label_names, yticklabels=label_names,\n",
        "            mask=mask)\n",
        "\n",
        "for i, text in enumerate(ax.texts):\n",
        "    row = i // num_classes\n",
        "    if np.sum(cm[row, :]) > 0:\n",
        "        text.set_text(text.get_text())\n",
        "    else:\n",
        "        text.set_text(\"N/A\")  # Mark cells in zero-support rows as N/A\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix \\nRows with N/A indicate gestures with no test samples')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "IPython.display.display(fig1)\n",
        "plt.close(fig1)\n",
        "\n",
        "\n",
        "# Calculate per-class metrics from our confusion matrix\n",
        "precision = np.zeros(num_classes)\n",
        "recall = np.zeros(num_classes)\n",
        "f1 = np.zeros(num_classes)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    # Precision: TP / (TP + FP)\n",
        "    col_sum = np.sum(cm[:, i])\n",
        "    if col_sum > 0:\n",
        "        precision[i] = cm[i, i] / col_sum\n",
        "    else:\n",
        "        precision[i] = 0 if class_counts[i] == 0 else 1  # If no predictions, precision is 0 (or 1 if no samples)\n",
        "\n",
        "    # Recall: TP / (TP + FN)\n",
        "    row_sum = np.sum(cm[i, :])\n",
        "    if row_sum > 0:\n",
        "        recall[i] = cm[i, i] / row_sum\n",
        "    else:\n",
        "        recall[i] = 0  # If no samples, recall is 0\n",
        "\n",
        "    # F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
        "    if precision[i] + recall[i] > 0:\n",
        "        f1[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
        "    else:\n",
        "        f1[i] = 0  # If precision and recall are 0, F1 is 0\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Gesture': label_names,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'Support': class_counts\n",
        "})\n",
        "\n",
        "print(\"\\nPer-class performance metrics (including zero-support gestures):\")\n",
        "print(metrics_df)\n",
        "\n",
        "\n",
        "# Create a bar chart showing support by class\n",
        "fig2 = plt.figure(figsize=(14, 6))\n",
        "plt.bar(label_names, class_counts)\n",
        "plt.title('Number of Test Samples per Gesture Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "IPython.display.display(fig2)\n",
        "plt.close(fig2)\n",
        "\n",
        "# Print important note about zero-support classes\n",
        "if zero_support_gestures:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"NOTE: The following gestures have no test samples: {', '.join(zero_support_gestures)}\")\n",
        "    print(\"These gestures appear in the confusion matrix as rows with all zeros (or N/A values).\")\n",
        "    print(\"Consider adding more test samples for these gestures for a more complete evaluation.\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "U0QyD_gHOYdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_found = False\n",
        "\n",
        "# Try different possible locations for history\n",
        "if hasattr(model, 'history_'):\n",
        "    history = model.history_\n",
        "    history_found = True\n",
        "elif hasattr(model, 'history'):\n",
        "    history = model.history\n",
        "    history_found = True\n",
        "elif hasattr(options, 'history'):\n",
        "    history = options.history\n",
        "    history_found = True\n",
        "\n",
        "# Plot training metrics if history was found\n",
        "if history_found and hasattr(history, 'history'):\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "    if 'loss' in history.history:\n",
        "        ax1.plot(history.history['loss'], label='Training Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "\n",
        "    ax1.set_title('Model Loss During Training')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    acc_key = 'accuracy' if 'accuracy' in history.history else 'acc'\n",
        "    val_acc_key = 'val_accuracy' if 'val_accuracy' in history.history else 'val_acc'\n",
        "\n",
        "    if acc_key in history.history:\n",
        "        ax2.plot(history.history[acc_key], label='Training Accuracy')\n",
        "    if val_acc_key in history.history:\n",
        "        ax2.plot(history.history[val_acc_key], label='Validation Accuracy')\n",
        "\n",
        "    ax2.set_title('Model Accuracy During Training')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Training history not available for plotting.\")\n",
        "\n",
        "    # Alternative: If we don't have history but this is in a notebook,\n",
        "    # we can pull the plot from TensorBoard if it was used\n",
        "    try:\n",
        "        from tensorboard import notebook\n",
        "        notebook.list() # Lists all TensorBoard instances\n",
        "        notebook.start(\"--logdir exported_model\")  # Try to find logs in the model directory\n",
        "    except:\n",
        "        print(\"TensorBoard visualization not available.\")"
      ],
      "metadata": {
        "id": "7YO1Oa-LSUhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "plt.close('all')\n",
        "\n",
        "# Get the labels and confusion matrix\n",
        "label_names = test_data.label_names\n",
        "num_classes = len(label_names)\n",
        "\n",
        "if 'cm' not in locals():\n",
        "    # Assuming true_labels exists from previous cell\n",
        "    if 'true_labels' not in locals():\n",
        "        true_labels = []\n",
        "        for batch in test_data.gen_tf_dataset(batch_size=1):\n",
        "            inputs, labels = batch\n",
        "            true_label = tf.argmax(labels, axis=1).numpy()[0]\n",
        "            true_labels.append(true_label)\n",
        "\n",
        "    if 'pred_labels' in locals() and len(pred_labels) == len(true_labels):\n",
        "        print(\"Using actual predictions\")\n",
        "        cm = confusion_matrix(true_labels, pred_labels, labels=range(num_classes))\n",
        "    else:\n",
        "        print(\"Using approximation based on overall accuracy\")\n",
        "        _, overall_accuracy = model.evaluate(test_data, batch_size=1)\n",
        "        print(f\"Overall accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "# Calculate precision for each class\n",
        "# Precision = TP / (TP + FP) = diagonal / column sum\n",
        "precision = np.zeros(num_classes)\n",
        "for i in range(num_classes):\n",
        "    col_sum = np.sum(cm[:, i])\n",
        "    if col_sum > 0:\n",
        "        precision[i] = cm[i, i] / col_sum\n",
        "    else:\n",
        "        precision[i] = 0\n",
        "\n",
        "precision_df = pd.DataFrame({\n",
        "    'Gesture': label_names,\n",
        "    'Precision': precision\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "\n",
        "precision_matrix = np.zeros_like(cm, dtype=float)\n",
        "for i in range(num_classes):\n",
        "    col_sum = np.sum(cm[:, i])\n",
        "    if col_sum > 0:\n",
        "        precision_matrix[:, i] = cm[:, i] / col_sum\n",
        "\n",
        "# heatmap\n",
        "sns.heatmap(precision_matrix, annot=True, fmt='.3f', cmap='Blues',\n",
        "            xticklabels=label_names, yticklabels=label_names)\n",
        "\n",
        "\n",
        "plt.title('Precision Confusion Matrix \\nRows with N/A indicate gestures with no test samples)')\n",
        "plt.xlabel('Predicted Label', fontsize=14)\n",
        "plt.ylabel('True Label', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CQO9NnY_VsbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yfN_47qjjOC"
      },
      "outputs": [],
      "source": [
        "files.download('exported_model/gesture_recognizer.task')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulqyNGmTCKeU"
      },
      "source": [
        "## Run the model on-device\n",
        "\n",
        "To use the TFLite model for on-device usage through MediaPipe Tasks, refer to the Gesture Recognizer [overview page](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1tiLGGRcvhy"
      },
      "source": [
        "## Hyperparameters {:#hyperparameters}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1UMEG85hQL_"
      },
      "source": [
        "You can further customize the model using the `GestureRecognizerOptions` class, which has two optional parameters for `ModelOptions` and `HParams`. Use the `ModelOptions` class to customize parameters related to the model itself, and the `HParams` class to customize other parameters related to training and saving the model.\n",
        "\n",
        "`ModelOptions` has one customizable parameter that affects accuracy:\n",
        "* `dropout_rate`: The fraction of the input units to drop. Used in dropout layer. Defaults to 0.05.\n",
        "* `layer_widths`: A list of hidden layer widths for the gesture model. Each element in the list will create a new hidden layer with the specified width. The hidden layers are separated with BatchNorm, Dropout, and ReLU. Defaults to an empty list(no hidden layers).\n",
        "\n",
        "`HParams` has the following list of customizable parameters which affect model accuracy:\n",
        "* `learning_rate`: The learning rate to use for gradient descent training. Defaults to 0.001.\n",
        "* `batch_size`: Batch size for training. Defaults to 2.\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 10.\n",
        "* `steps_per_epoch`: An optional integer that indicates the number of training steps per epoch. If not set, the training pipeline calculates the default steps per epoch as the training dataset size divided by batch size.\n",
        "* `shuffle`: True if the dataset is shuffled before training. Defaults to False.\n",
        "* `lr_decay`: Learning rate decay to use for gradient descent training. Defaults to 0.99.\n",
        "* `gamma`: Gamma parameter for focal loss. Defaults to 2\n",
        "\n",
        "Additional `HParams` parameter that does not affect model accuracy:\n",
        "* `export_dir`: The location of the model checkpoint files and exported model files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psvVZeSYBLfV"
      },
      "source": [
        "For example, the following trains a new model with the dropout_rate of 0.2 and learning rate of 0.003."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxMOI8o6iNLu"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir=\"exported_model_2\")\n",
        "model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)\n",
        "options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
        "model_2 = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cekuTJiBbv9"
      },
      "source": [
        "Evaluate the newly trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRH96bm-BbAo"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model_2.evaluate(test_data)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}